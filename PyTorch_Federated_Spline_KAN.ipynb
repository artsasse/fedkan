{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMF0hNYVR9HwNYZu615inPq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/artsasse/fedkan/blob/main/PyTorch_Federated_Spline_KAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalações"
      ],
      "metadata": {
        "id": "0h5M9JLbbv44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pykan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGa5u7cjHP-F",
        "outputId": "5769e21a-0c9b-4539-d7f5-b717092f6849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pykan\n",
            "  Downloading pykan-0.2.4-py3-none-any.whl.metadata (14 kB)\n",
            "Downloading pykan-0.2.4-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m997.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pykan\n",
            "Successfully installed pykan-0.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bibliotecas"
      ],
      "metadata": {
        "id": "nCnVOoSObz9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from kan import *"
      ],
      "metadata": {
        "id": "DhT5IBKJb1oE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funções"
      ],
      "metadata": {
        "id": "IZace0uI2ELI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gfz_RjAkUjPa"
      },
      "outputs": [],
      "source": [
        "def create_clients(image_list, label_list, num_clients=10, initial='client'):\n",
        "    ''' return: a dictionary with keys clients' names and value as\n",
        "                data shards - tuple of images and label lists.\n",
        "        args:\n",
        "            image_list: a list of numpy arrays of training images\n",
        "            label_list: a list of binarized labels for each image\n",
        "            num_client: number of federated members (clients)\n",
        "            initials: the clients' name prefix, e.g., client_1\n",
        "    '''\n",
        "    # Create a list of client names\n",
        "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
        "\n",
        "    # Randomize the data\n",
        "    data = list(zip(image_list, label_list))\n",
        "    random.shuffle(data)\n",
        "\n",
        "    # Shard data and place at each client\n",
        "    shard_size = len(data) // num_clients\n",
        "    shards = [data[i:i + shard_size] for i in range(0, shard_size * num_clients, shard_size)]\n",
        "\n",
        "    # Number of clients must equal number of shards\n",
        "    assert len(shards) == len(client_names)\n",
        "\n",
        "    return {client_names[i]: shards[i] for i in range(len(client_names))}\n",
        "\n",
        "\n",
        "def batch_data(data_shard, bs=32):\n",
        "    '''Takes in a client's data shard and creates a DataLoader object'''\n",
        "    data, label = zip(*data_shard)\n",
        "    data, label = torch.tensor(data, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
        "    dataset = TensorDataset(data, label)\n",
        "    return DataLoader(dataset, batch_size=bs, shuffle=True)\n",
        "\n",
        "\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 200)\n",
        "        self.layer2 = nn.Linear(200, 200)\n",
        "        self.layer3 = nn.Linear(200, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.softmax(self.layer3(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "def weight_scaling_factor(clients_trn_data, client_name):\n",
        "    client_names = list(clients_trn_data.keys())\n",
        "    bs = clients_trn_data[client_name].batch_size\n",
        "    global_count = sum([len(clients_trn_data[name].dataset) for name in client_names])\n",
        "    local_count = len(clients_trn_data[client_name].dataset)\n",
        "    return local_count / global_count\n",
        "\n",
        "\n",
        "def scale_model_weights(weight, scalar):\n",
        "    '''Function for scaling a model's weights'''\n",
        "    return [scalar * w for w in weight]\n",
        "\n",
        "\n",
        "def sum_scaled_weights(scaled_weight_list):\n",
        "    '''Return the sum of the listed scaled weights. This is equivalent to scaled avg of the weights'''\n",
        "    avg_grad = list()\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = torch.sum(torch.stack(grad_list_tuple), dim=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "    return avg_grad\n",
        "\n",
        "\n",
        "def test_model(X_test, Y_test, model, comm_round):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    with torch.no_grad():\n",
        "        logits = model(X_test)\n",
        "        loss = criterion(logits, torch.argmax(Y_test, dim=1))\n",
        "        acc = accuracy_score(torch.argmax(logits, axis=1).numpy(), torch.argmax(Y_test, axis=1).numpy())\n",
        "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss.item()))\n",
        "    return acc, loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execução"
      ],
      "metadata": {
        "id": "cZJrMyuG2Hpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO Assure notebook reproducibility (read Kalinowski's book)\n",
        "\n",
        "# Load MNIST dataset\n",
        "# Temporarily using dataset from Keras, to mantain uniformity with the baseline\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "X_train = X_train.reshape(-1, 28 * 28) / 255.0\n",
        "X_test = X_test.reshape(-1, 28 * 28) / 255.0\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "\n",
        "# Create clients\n",
        "clients = create_clients(X_train, y_train, num_clients=10, initial='client')\n",
        "\n",
        "# Process and batch the training data for each client\n",
        "clients_batched = {client_name: batch_data(data) for client_name, data in clients.items()}\n",
        "\n",
        "\n",
        "# Convert data to PyTorch tensors (need to do it after the clients batching because of typing errors)\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Process and batch the test set\n",
        "#test_batched = DataLoader(TensorDataset(X_test, y_test), batch_size=len(y_test), shuffle=False)\n",
        "\n",
        "\n",
        "# Define number of communication rounds\n",
        "comms_round = 3\n",
        "\n",
        "# Initialize global model\n",
        "global_model = SimpleMLP(784, 10)\n",
        "global_model_weights = list(global_model.parameters())\n",
        "\n",
        "# Start global training loop\n",
        "print(\"Federated Model Results:\")\n",
        "for comm_round in range(1,comms_round+1):\n",
        "\n",
        "    # Initial list to collect local model weights after scaling\n",
        "    scaled_local_weight_list = []\n",
        "\n",
        "    # Randomize client data - using keys\n",
        "    client_names = list(clients_batched.keys())\n",
        "    random.shuffle(client_names)\n",
        "\n",
        "    # Loop through each client and create new local model\n",
        "    for client in client_names:\n",
        "        local_model = SimpleMLP(784, 10)\n",
        "        local_model.load_state_dict(global_model.state_dict())\n",
        "\n",
        "        # Create a new optimizer instance for each local model\n",
        "        lr = 0.01\n",
        "        optimizer = optim.SGD(local_model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "        # Train local model\n",
        "        local_model.train()\n",
        "        for X_batch, y_batch in clients_batched[client]:\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = local_model(X_batch)\n",
        "            loss = nn.CrossEntropyLoss()(y_pred, torch.argmax(y_batch, dim=1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Scale the model weights and add to list\n",
        "        scaling_factor = weight_scaling_factor(clients_batched, client)\n",
        "        scaled_weights = scale_model_weights(list(local_model.parameters()), scaling_factor)\n",
        "        scaled_local_weight_list.append(scaled_weights)\n",
        "\n",
        "    # To get the average over all the local models, we simply take the sum of the scaled weights\n",
        "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
        "\n",
        "    # Update global model\n",
        "    with torch.no_grad():\n",
        "        for global_param, avg_weight in zip(global_model.parameters(), average_weights):\n",
        "            global_param.data = avg_weight\n",
        "\n",
        "    # Test global model and print out metrics after each communication round\n",
        "    for X_test_batch, Y_test_batch in test_batched:\n",
        "        global_acc, global_loss = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Prepare the SGD dataset\n",
        "SGD_dataset = DataLoader(TensorDataset(X_train, y_train), shuffle=True, batch_size=320)\n",
        "\n",
        "# Initialize the SGD model\n",
        "SGD_model = SimpleMLP(784, 10)\n",
        "lr = 0.01\n",
        "optimizer = optim.SGD(SGD_model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "# Train the SGD model\n",
        "SGD_model.train()\n",
        "for epoch in range(3):\n",
        "    for X_batch, y_batch in SGD_dataset:\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = SGD_model(X_batch)\n",
        "        loss = nn.CrossEntropyLoss()(y_pred, torch.argmax(y_batch, dim=1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Test the SGD global model and print out metrics\n",
        "SGD_model.eval()\n",
        "with torch.no_grad():\n",
        "    print(\"Centralized Model Results:\")\n",
        "    for X_test_batch, Y_test_batch in test_batched:\n",
        "        SGD_acc, SGD_loss = test_model(X_test_batch, Y_test_batch, SGD_model, 1)"
      ],
      "metadata": {
        "id": "TIkaqAHi2JSq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "6f87ffbc-8bb7-4bc4-c991-097b1bfb9b4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint directory created: ./model\n",
            "saving model version 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " train_loss: 8.14e+01 | reg: 3.23e+05 | train_acc: 9.01e-01 | test_acc: 9.01e-01 |: 100%|█| 3/3 [03:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving model version 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Prepare the SGD dataset\\nSGD_dataset = DataLoader(TensorDataset(X_train, y_train), shuffle=True, batch_size=320)\\n\\n# Initialize the SGD model\\nSGD_model = SimpleMLP(784, 10)\\nlr = 0.01\\noptimizer = optim.SGD(SGD_model.parameters(), lr=lr, momentum=0.9)\\n\\n# Train the SGD model\\nSGD_model.train()\\nfor epoch in range(3):\\n    for X_batch, y_batch in SGD_dataset:\\n        optimizer.zero_grad()\\n        y_pred = SGD_model(X_batch)\\n        loss = nn.CrossEntropyLoss()(y_pred, torch.argmax(y_batch, dim=1))\\n        loss.backward()\\n        optimizer.step()\\n\\n# Test the SGD global model and print out metrics\\nSGD_model.eval()\\nwith torch.no_grad():\\n    print(\"Centralized Model Results:\")\\n    for X_test_batch, Y_test_batch in test_batched:\\n        SGD_acc, SGD_loss = test_model(X_test_batch, Y_test_batch, SGD_model, 1)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Toy Example com PyKAN"
      ],
      "metadata": {
        "id": "v14FxvUqYfoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "# Temporarily using dataset from Keras, to mantain uniformity with the baseline\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "X_train = X_train.reshape(-1, 28 * 28) / 255.0\n",
        "X_test = X_test.reshape(-1, 28 * 28) / 255.0\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Convert data to PyTorch tensors (need to do it after the clients batching because of typing errors)\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "dataset = {}\n",
        "dataset['train_input'] = X_train\n",
        "dataset['test_input'] = X_test\n",
        "dataset['train_label'] = y_train\n",
        "dataset['test_label'] = y_test\n",
        "\n",
        "n_features = X_train.shape[1]\n",
        "n_classes = y_train.shape[1]\n",
        "\n",
        "model = KAN(width=[n_features, 3, n_classes], grid=3, k=3, seed=42)\n",
        "\n",
        "def train_acc():\n",
        "    return torch.mean((torch.argmax((model(dataset['train_input'])), dim=1) == torch.argmax(dataset['train_label'])).type(torch.float32))\n",
        "\n",
        "def test_acc():\n",
        "    return torch.mean((torch.argmax((model(dataset['test_input'])), dim=1) == torch.argmax(dataset['test_label'])).type(torch.float32))\n",
        "\n",
        "model.fit(dataset, steps=3, lamb=0.005, batch=1024, loss_fn = nn.CrossEntropyLoss(), metrics=[train_acc, test_acc], display_metrics=['train_loss', 'reg', 'train_acc', 'test_acc'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxSqASOrYjJQ",
        "outputId": "3a46db90-e3d2-422b-a226-4ed839c89a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint directory created: ./model\n",
            "saving model version 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " train_loss: 8.14e+01 | reg: 3.23e+05 | train_acc: 0.00e+00 | test_acc: 4.00e-04 |: 100%|█| 3/3 [03:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving model version 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train_loss': [array(1.5174258, dtype=float32),\n",
              "  array(1.5174273, dtype=float32),\n",
              "  array(81.4046, dtype=float32)],\n",
              " 'test_loss': [array(79.32478, dtype=float32),\n",
              "  array(1.5166861, dtype=float32),\n",
              "  array(87.555984, dtype=float32)],\n",
              " 'reg': [array(2597.382, dtype=float32),\n",
              "  array(291.35287, dtype=float32),\n",
              "  array(322988.97, dtype=float32)],\n",
              " 'train_acc': [0.000733333348762244, 0.0, 0.0],\n",
              " 'test_acc': [0.00039999998989515007, 0.0, 0.00039999998989515007]}"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    }
  ]
}