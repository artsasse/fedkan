{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJAq8a6wYVbrldd+IC1kXN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/artsasse/fedkan/blob/main/PyTorch_Federated_SGD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funções"
      ],
      "metadata": {
        "id": "IZace0uI2ELI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Gfz_RjAkUjPa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def create_clients(image_list, label_list, num_clients=10, initial='client'):\n",
        "    ''' return: a dictionary with keys clients' names and value as\n",
        "                data shards - tuple of images and label lists.\n",
        "        args:\n",
        "            image_list: a list of numpy arrays of training images\n",
        "            label_list: a list of binarized labels for each image\n",
        "            num_client: number of federated members (clients)\n",
        "            initials: the clients' name prefix, e.g., client_1\n",
        "    '''\n",
        "    # Create a list of client names\n",
        "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
        "\n",
        "    # Randomize the data\n",
        "    data = list(zip(image_list, label_list))\n",
        "    random.shuffle(data)\n",
        "\n",
        "    # Shard data and place at each client\n",
        "    shard_size = len(data) // num_clients\n",
        "    shards = [data[i:i + shard_size] for i in range(0, shard_size * num_clients, shard_size)]\n",
        "\n",
        "    # Number of clients must equal number of shards\n",
        "    assert len(shards) == len(client_names)\n",
        "\n",
        "    return {client_names[i]: shards[i] for i in range(len(client_names))}\n",
        "\n",
        "\n",
        "def batch_data(data_shard, bs=32):\n",
        "    '''Takes in a client's data shard and creates a DataLoader object'''\n",
        "    data, label = zip(*data_shard)\n",
        "    data, label = torch.tensor(data, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
        "    dataset = TensorDataset(data, label)\n",
        "    return DataLoader(dataset, batch_size=bs, shuffle=True)\n",
        "\n",
        "\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 200)\n",
        "        self.layer2 = nn.Linear(200, 200)\n",
        "        self.layer3 = nn.Linear(200, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.softmax(self.layer3(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "def weight_scaling_factor(clients_trn_data, client_name):\n",
        "    client_names = list(clients_trn_data.keys())\n",
        "    bs = clients_trn_data[client_name].batch_size\n",
        "    global_count = sum([len(clients_trn_data[name].dataset) for name in client_names])\n",
        "    local_count = len(clients_trn_data[client_name].dataset)\n",
        "    return local_count / global_count\n",
        "\n",
        "\n",
        "def scale_model_weights(weight, scalar):\n",
        "    '''Function for scaling a model's weights'''\n",
        "    return [scalar * w for w in weight]\n",
        "\n",
        "\n",
        "def sum_scaled_weights(scaled_weight_list):\n",
        "    '''Return the sum of the listed scaled weights. This is equivalent to scaled avg of the weights'''\n",
        "    avg_grad = list()\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = torch.sum(torch.stack(grad_list_tuple), dim=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "    return avg_grad\n",
        "\n",
        "\n",
        "def test_model(X_test, Y_test, model, comm_round):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    with torch.no_grad():\n",
        "        logits = model(X_test)\n",
        "        loss = criterion(logits, torch.argmax(Y_test, dim=1))\n",
        "        acc = accuracy_score(torch.argmax(logits, axis=1).numpy(), torch.argmax(Y_test, axis=1).numpy())\n",
        "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss.item()))\n",
        "    return acc, loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execução"
      ],
      "metadata": {
        "id": "cZJrMyuG2Hpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO Assure notebook reproducibility (read Kalinowski's book)\n",
        "\n",
        "# Load MNIST dataset\n",
        "# Temporarily using dataset from Keras, to mantain uniformity with the baseline\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "X_train = X_train.reshape(-1, 28 * 28) / 255.0\n",
        "X_test = X_test.reshape(-1, 28 * 28) / 255.0\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Create clients\n",
        "clients = create_clients(X_train, y_train, num_clients=10, initial='client')\n",
        "\n",
        "# Process and batch the training data for each client\n",
        "clients_batched = {client_name: batch_data(data) for client_name, data in clients.items()}\n",
        "\n",
        "# Convert data to PyTorch tensors (need to do it after the clients batching because of typing errors)\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Process and batch the test set\n",
        "test_batched = DataLoader(TensorDataset(X_test, y_test), batch_size=len(y_test), shuffle=False)\n",
        "\n",
        "# Define number of communication rounds\n",
        "comms_round = 3\n",
        "\n",
        "# Initialize global model\n",
        "global_model = SimpleMLP(784, 10)\n",
        "global_model_weights = list(global_model.parameters())\n",
        "\n",
        "# Start global training loop\n",
        "print(\"Federated Model Results:\")\n",
        "for comm_round in range(1,comms_round+1):\n",
        "\n",
        "    # Initial list to collect local model weights after scaling\n",
        "    scaled_local_weight_list = []\n",
        "\n",
        "    # Randomize client data - using keys\n",
        "    client_names = list(clients_batched.keys())\n",
        "    random.shuffle(client_names)\n",
        "\n",
        "    # Loop through each client and create new local model\n",
        "    for client in client_names:\n",
        "        local_model = SimpleMLP(784, 10)\n",
        "        local_model.load_state_dict(global_model.state_dict())\n",
        "\n",
        "        # Create a new optimizer instance for each local model\n",
        "        lr = 0.01\n",
        "        optimizer = optim.SGD(local_model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "        # Train local model\n",
        "        local_model.train()\n",
        "        for X_batch, y_batch in clients_batched[client]:\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = local_model(X_batch)\n",
        "            loss = nn.CrossEntropyLoss()(y_pred, torch.argmax(y_batch, dim=1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Scale the model weights and add to list\n",
        "        scaling_factor = weight_scaling_factor(clients_batched, client)\n",
        "        scaled_weights = scale_model_weights(list(local_model.parameters()), scaling_factor)\n",
        "        scaled_local_weight_list.append(scaled_weights)\n",
        "\n",
        "    # To get the average over all the local models, we simply take the sum of the scaled weights\n",
        "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
        "\n",
        "    # Update global model\n",
        "    with torch.no_grad():\n",
        "        for global_param, avg_weight in zip(global_model.parameters(), average_weights):\n",
        "            global_param.data = avg_weight\n",
        "\n",
        "    # Test global model and print out metrics after each communication round\n",
        "    for X_test_batch, Y_test_batch in test_batched:\n",
        "        global_acc, global_loss = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
        "\n",
        "# Prepare the SGD dataset\n",
        "SGD_dataset = DataLoader(TensorDataset(X_train, y_train), shuffle=True, batch_size=320)\n",
        "\n",
        "# Initialize the SGD model\n",
        "SGD_model = SimpleMLP(784, 10)\n",
        "lr = 0.01\n",
        "optimizer = optim.SGD(SGD_model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "# Train the SGD model\n",
        "SGD_model.train()\n",
        "for epoch in range(3):\n",
        "    for X_batch, y_batch in SGD_dataset:\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = SGD_model(X_batch)\n",
        "        loss = nn.CrossEntropyLoss()(y_pred, torch.argmax(y_batch, dim=1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Test the SGD global model and print out metrics\n",
        "SGD_model.eval()\n",
        "with torch.no_grad():\n",
        "    print(\"Centralized Model Results:\")\n",
        "    for X_test_batch, Y_test_batch in test_batched:\n",
        "        SGD_acc, SGD_loss = test_model(X_test_batch, Y_test_batch, SGD_model, 1)\n"
      ],
      "metadata": {
        "id": "TIkaqAHi2JSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acaacaa8-c856-4b97-a921-9d371b11976e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Federated Model Results:\n",
            "comm_round: 1 | global_acc: 27.770% | global_loss: 2.2952542304992676\n",
            "comm_round: 2 | global_acc: 25.550% | global_loss: 2.2460381984710693\n",
            "comm_round: 3 | global_acc: 62.930% | global_loss: 1.9492806196212769\n",
            "Centralized Model Results:\n",
            "comm_round: 1 | global_acc: 56.070% | global_loss: 1.9219034910202026\n"
          ]
        }
      ]
    }
  ]
}